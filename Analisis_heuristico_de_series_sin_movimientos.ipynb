{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F_oP0868u6U"
      },
      "source": [
        "# Analisis Heuristico de Equipos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdU4JBsu8yNl"
      },
      "source": [
        "## Librerias y variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rZoJjhSNQ0jq"
      },
      "outputs": [],
      "source": [
        "# === üìö Importacion de librer√≠as ===\n",
        "import csv\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta, date\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nlXabQ9NP-xp"
      },
      "outputs": [],
      "source": [
        "# === ‚öôÔ∏è Variables generales ===\n",
        "archivo_Series = \"series_frappe.csv\"\n",
        "archivo_crudo = \"crudo.csv\"\n",
        "archivo_seriesFiltrados = \"series_filtrados.csv\"\n",
        "\n",
        "# URL directa del CSV (sin filtro)\n",
        "url_crudo_docs = userdata.get('planilla_anual')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "enviar=1\n",
        "\n",
        "\n",
        "# === üîê API ERP ===\n",
        "API_KEY = userdata.get('API_KEY')\n",
        "API_SECRET = userdata.get('API_SECRET')\n",
        "try:\n",
        "  FRAPPE_API_crudo = requests.get(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRbnN152KxnzZqA3GA0L0LQJCF9uGRNIXsydi_FB1EveJ_aVSD_5rjHtTcfoGyctN8ZPou5R7Eg-QQ8/pub?gid=1381182595&single=true&output=csv\").text.splitlines()[1]\n",
        "  FRAPPE_API=FRAPPE_API_crudo + \"/api/resource\"\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Error fatal: No se pudo obtener la URL de la API de Frappe desde Google Docs. Revisa el enlace. Error: {e}\")\n",
        "    FRAPPE_API = None # Dejar la API como None para detener la ejecuci√≥n si falla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK2KfFaj9JJA"
      },
      "source": [
        "## Funciones menores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hfHxrK0w9Lbw"
      },
      "outputs": [],
      "source": [
        "def agregar_comentario(doctype: str, docname: str, contenido: str, remitente: str = \"aguida@qpsrl.com.ar\"):\n",
        "    if not FRAPPE_API_crudo:\n",
        "        print(\"‚ùå No se puede agregar comentario, la URL de la API de Frappe no est√° configurada.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        url = f\"{FRAPPE_API_crudo}/api/method/frappe.desk.form.utils.add_comment\"\n",
        "        payload = {\n",
        "            \"reference_doctype\": doctype, \"reference_name\": docname, \"content\": contenido,\n",
        "            \"comment_email\": remitente, \"comment_by\": remitente\n",
        "        }\n",
        "        headers = {\"Authorization\": f\"token {API_KEY}:{API_SECRET}\", \"Content-Type\": \"application/json\"}\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        print(f\"üó®Ô∏è Comentario agregado exitosamente a {docname}.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_text = e.response.text if e.response else \"Sin respuesta del servidor\"\n",
        "        print(f\"‚ùå Error al enviar comentario a Frappe para {docname}: {error_text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error inesperado en agregar_comentario: {e}\")\n",
        "\n",
        "def crear_delivery_note_por_serie(serie_detectada, fila_crudo, enviar=1):\n",
        "    if not FRAPPE_API: return None\n",
        "\n",
        "    dic_series = {row[\"name\"].strip(): {\"item_code\": row[\"item_code\"].strip(),\n",
        "                                       \"item_name\": row[\"item_name\"].strip(),\n",
        "                                       \"warehouse\": row[\"warehouse\"].strip()}\n",
        "                  for row in csv.DictReader(open(archivo_Series, newline='', encoding=\"utf-8\"))}\n",
        "\n",
        "    info_serie = dic_series.get(serie_detectada)\n",
        "\n",
        "    # Verificaci√≥n adicional (aunque ya deber√≠a estar filtrado)\n",
        "    if not info_serie or not info_serie[\"warehouse\"]:\n",
        "        print(f\"‚ö†Ô∏è No se puede crear DN para '{serie_detectada}'. No se encontr√≥ o no tiene almac√©n.\")\n",
        "        return None\n",
        "\n",
        "    payload = {\n",
        "        \"customer\": \"Instalado\", \"posting_date\": str(date.today()), \"set_warehouse\": info_serie[\"warehouse\"],\n",
        "        \"docstatus\": enviar, \"items\": [{\"item_code\": info_serie[\"item_code\"], \"item_name\": info_serie[\"item_name\"],\n",
        "                                  \"qty\": 1, \"warehouse\": info_serie[\"warehouse\"], \"serial_no\": serie_detectada}],\n",
        "    }\n",
        "\n",
        "    FRAPPE_API_delivery = f\"{FRAPPE_API}/Delivery%20Note\"\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"ngrok-skip-browser-warning\": \"true\"\n",
        "    }\n",
        "\n",
        "    print(f\"üì¶ Creando Delivery Note para la serie '{serie_detectada}'...\")\n",
        "    try:\n",
        "        # Crear sesi√≥n con retries\n",
        "        session = requests.Session()\n",
        "        session.headers.update(HEADERS)\n",
        "\n",
        "        response = session.post(FRAPPE_API_delivery, json=payload)\n",
        "        response.raise_for_status()\n",
        "        docname = response.json()[\"data\"][\"name\"]\n",
        "        print(f\"‚úÖ Delivery Note {FRAPPE_API_crudo}/app/delivery-note/{docname} creada exitosamente.\")\n",
        "\n",
        "        headers = \"\".join(f\"<th style='padding: 5px; border: 1px solid #ccc;'>{col}</th>\" for col in fila_crudo.keys())\n",
        "        values  = \"\".join(f\"<td style='padding: 5px; border: 1px solid #ccc;'>{val}</td>\" for val in fila_crudo.values())\n",
        "\n",
        "        comentario_tabla = (\n",
        "            \"<b>Detalle de la fila encontrada en el archivo crudo de instalaciones:</b><br>\"\n",
        "            \"<table border='1' style='width:100%; border-collapse: collapse; margin-top: 10px;'>\"\n",
        "            f\"<tr>{headers}</tr>\"\n",
        "            f\"<tr>{values}</tr>\"\n",
        "            \"</table>\"\n",
        "        )\n",
        "\n",
        "        agregar_comentario(doctype=\"Delivery Note\", docname=docname, contenido=comentario_tabla)\n",
        "\n",
        "        comentario_automatico = \"<p style='margin-top:15px;'>Este documento fue generado autom√°ticamente por el script de trazabilidad debido a una <b>coincidencia exacta</b> del n√∫mero de serie.</p>\"\n",
        "        agregar_comentario(doctype=\"Delivery Note\", docname=docname, contenido=comentario_automatico)\n",
        "\n",
        "        return docname\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error creando Delivery Note para '{serie_detectada}': {e.response.text if e.response else e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV4HgNTT82OW"
      },
      "source": [
        "## Funciones de obtencion de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iHksuc2-860R"
      },
      "outputs": [],
      "source": [
        "# === üõ†Ô∏è Funciones de Descarga y Preparaci√≥n ===\n",
        "\n",
        "def descargar_crudo_completo():\n",
        "    print(\"Descargando archivo crudo de instalaciones...\")\n",
        "    try:\n",
        "        resp = requests.get(url_crudo_docs)\n",
        "        resp.raise_for_status()\n",
        "        with open(archivo_crudo, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "            f.write(resp.text)\n",
        "        print(f\"‚úÖ Crudo completo guardado en '{archivo_crudo}'\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error al descargar el archivo crudo: {e}\")\n",
        "        return False\n",
        "\n",
        "def obtener_series_frappe(lista_nombres=None):\n",
        "    if not FRAPPE_API:\n",
        "        return False\n",
        "\n",
        "    campos = [\n",
        "        \"name\", \"docstatus\", \"item_code\", \"item_name\",\n",
        "        \"warehouse\", \"creation\", \"modified\", \"_user_tags\"\n",
        "    ]\n",
        "\n",
        "    # Si se proporciona una lista de nombres, crear registros para cada uno\n",
        "    if lista_nombres and isinstance(lista_nombres, list) and len(lista_nombres) > 0:\n",
        "        print(f\"Creando archivo con {len(lista_nombres)} series de la lista...\")\n",
        "        data = []\n",
        "        for nombre in lista_nombres:\n",
        "            registro = {campo: \"\" for campo in campos}  # Crear registro vac√≠o\n",
        "            registro[\"name\"] = nombre  # Llenar el campo name\n",
        "            registro[\"warehouse\"] = \"1\"  # Poner \"1\" en warehouse para series de la lista\n",
        "            data.append(registro)\n",
        "    else:\n",
        "        # Comportamiento normal: obtener todos los series de Frappe\n",
        "        print(\"Consultando todas las series en Frappe...\")\n",
        "        FRAPPE_API_serie = FRAPPE_API + \"/Serial%20No\"\n",
        "        HEADERS = {\n",
        "            \"Authorization\": f\"token {API_KEY}:{API_SECRET}\"\n",
        "        }\n",
        "\n",
        "        order_by_param = \"modified asc\"\n",
        "        fields_param = json.dumps(campos)\n",
        "        url = f\"{FRAPPE_API_serie}?fields={fields_param}&limit_page_length=30000&order_by={order_by_param}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"data\", [])\n",
        "        except (requests.exceptions.RequestException, json.JSONDecodeError) as e:\n",
        "            print(f\"‚ùå Error obteniendo series de Frappe: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Escribir el archivo CSV\n",
        "    try:\n",
        "        with open(archivo_Series, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=campos)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(data)\n",
        "\n",
        "        print(f\"‚úÖ {len(data)} series guardadas en '{archivo_Series}'\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error escribiendo archivo CSV: {e}\")\n",
        "        return False\n",
        "\n",
        "# === üß† Funciones de Trazabilidad y An√°lisis ===\n",
        "\n",
        "def obtener_series_filtradas(filtro_tiempo=False):\n",
        "    series_a_buscar = []\n",
        "    datos_para_csv = []\n",
        "    dos_semanas_atras = datetime.now() - timedelta(weeks=2)\n",
        "\n",
        "    with open(archivo_Series, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for fila in reader:\n",
        "            warehouse = fila.get(\"warehouse\", \"\").strip()\n",
        "            serie = fila[\"name\"].strip().upper()\n",
        "\n",
        "            # üî¥ Siempre descartar si no hay warehouse\n",
        "            if not warehouse:\n",
        "                continue\n",
        "\n",
        "            cumple_filtro_tiempo = True\n",
        "            if filtro_tiempo:\n",
        "                try:\n",
        "                    # New, correct date format: \"%Y-%m-%d %H:%M:%S.%f\"\n",
        "                    fecha_mod = datetime.strptime(fila[\"modified\"], \"%Y-%m-%d %H:%M:%S.%f\")\n",
        "                    if fecha_mod >= dos_semanas_atras:\n",
        "                        cumple_filtro_tiempo = False\n",
        "                except (ValueError, TypeError):\n",
        "                    # Handle cases where the date format is different or the field is empty\n",
        "                    # For safety, we'll assume it doesn't meet the time filter if we can't parse it.\n",
        "                    cumple_filtro_tiempo = False\n",
        "                    print(f\"‚ö†Ô∏è Advertencia: No se pudo parsear la fecha '{fila.get('modified', 'N/A')}' para la serie {serie}. No se incluir√° en el filtro de tiempo.\")\n",
        "\n",
        "            if cumple_filtro_tiempo:\n",
        "                series_a_buscar.append(serie)\n",
        "                datos_para_csv.append({\n",
        "                    \"serie\": serie, \"almacen\": warehouse, \"item_code\": fila[\"item_code\"],\n",
        "                    \"item_name\": fila[\"item_name\"], \"modified\": fila[\"modified\"]\n",
        "                })\n",
        "\n",
        "    with open(archivo_seriesFiltrados, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.DictWriter(f_out, fieldnames=[\"serie\", \"almacen\", \"item_code\", \"item_name\", \"modified\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(datos_para_csv)\n",
        "\n",
        "    print(f\"üéØ Se prepararon {len(series_a_buscar)} series para la trazabilidad (todas con almac√©n no vac√≠o).\")\n",
        "    return series_a_buscar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhGQO6608-Jl"
      },
      "source": [
        "## Funciones de busqueda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Evm8BS33glix"
      },
      "outputs": [],
      "source": [
        "def buscar_series_en_crudo(series_buscar):\n",
        "    encontrados = {}\n",
        "    series_a_buscar_set = set(series_buscar)\n",
        "\n",
        "    with open(archivo_crudo, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)  # Usar DictReader en lugar de reader\n",
        "        for fila in reader:\n",
        "            # Obtener los valores de las columnas por nombre en lugar de √≠ndice\n",
        "            columna1 = fila.get('SERIAL ONT', '')\n",
        "            columna9 = fila.get('SERIALES DECO FLOW', '')\n",
        "            columna10 = fila.get('COMENTARIOS DE CIERRE', '')\n",
        "\n",
        "            texto_crudo = \" \".join([columna1, columna9, columna10]).upper()\n",
        "\n",
        "            for serie in series_a_buscar_set:\n",
        "                if serie in texto_crudo and serie not in encontrados:\n",
        "                    encontrados[serie] = fila\n",
        "\n",
        "    print(f\"‚úîÔ∏è Se encontraron {len(encontrados)} coincidencias exactas en el archivo crudo.\")\n",
        "    return encontrados\n",
        "\n",
        "def distancia_levenshtein(a, b):\n",
        "    if len(a) < len(b): return distancia_levenshtein(b, a)\n",
        "    if len(b) == 0: return len(a)\n",
        "    previous_row = range(len(b) + 1)\n",
        "    for i, c1 in enumerate(a):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(b):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def buscar_series_en_crudo_heuristico(series_buscar, max_distancia=3):\n",
        "    sugerencias = []\n",
        "    series_frappe_info = {}\n",
        "\n",
        "    # Leer todas las series de Frappe para verificar existencia y almac√©n\n",
        "    with open(archivo_Series, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            series_frappe_info[row['name'].strip().upper()] = row['warehouse'].strip()\n",
        "\n",
        "    with open(archivo_crudo, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader, None)\n",
        "        for i, fila in enumerate(reader):\n",
        "            if len(fila) < 11: continue\n",
        "            texto_crudo = \" \".join([fila[1], fila[9], fila[10]]).upper()\n",
        "            imagenes = fila[8].strip().split(\",\") if len(fila) > 8 else []\n",
        "            palabras = set(re.findall(r\"[A-Z0-9]{8,}\", texto_crudo))\n",
        "\n",
        "            for serie_objetivo in series_buscar:\n",
        "                for palabra in palabras:\n",
        "                    if serie_objetivo == palabra:\n",
        "                        continue\n",
        "\n",
        "                    dist = distancia_levenshtein(serie_objetivo, palabra)\n",
        "                    if 0 < dist <= max_distancia:\n",
        "                        # VALIDACI√ìN MODIFICADA seg√∫n los nuevos requisitos\n",
        "                        warehouse_de_palabra = series_frappe_info.get(palabra)\n",
        "\n",
        "                        # Solo considerar si:\n",
        "                        # 1. La palabra NO existe en la base de datos (warehouse_de_palabra es None)\n",
        "                        # O 2. Si existe, debe tener un warehouse no vac√≠o\n",
        "                        if warehouse_de_palabra is not None and not warehouse_de_palabra:\n",
        "                            continue  # Existe pero tiene warehouse vac√≠o, descartar\n",
        "\n",
        "                        confianza = round(1 - dist / max(len(serie_objetivo), len(palabra)), 2)\n",
        "                        sugerencias.append({\n",
        "                            \"serie_buscada\": serie_objetivo,\n",
        "                            \"warehouse_buscada\": series_frappe_info.get(serie_objetivo, \"N/A\"),\n",
        "                            \"posible_coincidencia\": palabra,\n",
        "                            \"distancia\": dist,\n",
        "                            \"confianza_percent\": int(confianza * 100),\n",
        "                            \"fila_crudo_num\": i + 2,\n",
        "                            \"imagenes\": [img.strip() for img in imagenes if img.strip()]\n",
        "                        })\n",
        "\n",
        "    sugerencias.sort(key=lambda x: x[\"confianza_percent\"], reverse=True)\n",
        "    print(f\"\\nü§ñ Se generaron {len(sugerencias)} sugerencias heur√≠sticas (distancia <= {max_distancia}):\\n\")\n",
        "\n",
        "    for s in sugerencias:\n",
        "        print(f\" - Serie Buscada: {s['serie_buscada']} (Almac√©n: {s['warehouse_buscada']})\")\n",
        "        print(f\"   Posible Coincidencia: {s['posible_coincidencia']}\")\n",
        "        print(f\"   Posibilidad de match: {s['confianza_percent']}% (Distancia: {s['distancia']})\")\n",
        "        print(f\"   Ubicaci√≥n en Crudo: Fila {s['fila_crudo_num']} | Im√°genes: {s['imagenes']}\\n\")\n",
        "\n",
        "    return sugerencias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb_UbMrV9nCz"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIccniJV9Ym1",
        "outputId": "fdd06721-114c-47dd-9727-9998513ae9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================================\n",
            "|| EJECUTANDO AN√ÅLISIS DE SERIES CON ALMAC√âN NO VAC√çO ||\n",
            "========================================================\n",
            "\n",
            "--- INICIANDO AN√ÅLISIS DE TRAZABILIDAD ---\n",
            "Modo de operaci√≥n: Buscando todas las series con stock.\n",
            "Distancia heur√≠stica m√°xima establecida en: 2\n",
            "Descargando archivo crudo de instalaciones...\n",
            "‚úÖ Crudo completo guardado en 'crudo.csv'\n",
            "Consultando todas las series en Frappe...\n",
            "‚úÖ 2193 series guardadas en 'series_frappe.csv'\n",
            "\n",
            "--- ETAPA 1: FILTRANDO SERIES A BUSCAR ---\n",
            "üéØ Se prepararon 155 series para la trazabilidad (todas con almac√©n no vac√≠o).\n",
            "\n",
            "--- ETAPA 2: BUSCANDO COINCIDENCIAS EXACTAS Y CREANDO DELIVERY NOTES ---\n",
            "‚úîÔ∏è Se encontraron 2 coincidencias exactas en el archivo crudo.\n",
            "\n",
            "Procesando coincidencia exacta para: FZ324060356542\n",
            "‚úÖ ¬°Atenci√≥n! Se encontr√≥ una posible Delivery Note para la serie FZ324060356542, pero la fecha de registro (2025-10-20) es del d√≠a de hoy. No se crear√° la Delivery Note.\n",
            "\n",
            "Procesando coincidencia exacta para: FZ324060367339\n",
            "‚úÖ ¬°Atenci√≥n! Se encontr√≥ una posible Delivery Note para la serie FZ324060367339, pero la fecha de registro (2025-10-20) es del d√≠a de hoy. No se crear√° la Delivery Note.\n",
            "\n",
            "--- ETAPA 3: BUSCANDO SUGERENCIAS HEUR√çSTICAS (ERRORES DE TIPEO) ---\n",
            "153 series no tuvieron coincidencia exacta. Iniciando b√∫squeda heur√≠stica...\n",
            "\n",
            "ü§ñ Se generaron 0 sugerencias heur√≠sticas (distancia <= 2):\n",
            "\n",
            "\n",
            "‚úÖ Proceso de trazabilidad completado.\n",
            "--- INICIANDO AN√ÅLISIS DE TRAZABILIDAD ---\n",
            "Modo de operaci√≥n: Buscando series sin movimiento en 2 semanas.\n",
            "Distancia heur√≠stica m√°xima establecida en: 2\n",
            "Descargando archivo crudo de instalaciones...\n",
            "‚úÖ Crudo completo guardado en 'crudo.csv'\n",
            "Consultando todas las series en Frappe...\n",
            "‚úÖ 2193 series guardadas en 'series_frappe.csv'\n",
            "\n",
            "--- ETAPA 1: FILTRANDO SERIES A BUSCAR ---\n",
            "üéØ Se prepararon 19 series para la trazabilidad (todas con almac√©n no vac√≠o).\n",
            "\n",
            "--- ETAPA 2: BUSCANDO COINCIDENCIAS EXACTAS Y CREANDO DELIVERY NOTES ---\n",
            "‚úîÔ∏è Se encontraron 1 coincidencias exactas en el archivo crudo.\n",
            "\n",
            "Procesando coincidencia exacta para: FZ324060356542\n",
            "‚úÖ ¬°Atenci√≥n! Se encontr√≥ una posible Delivery Note para la serie FZ324060356542, pero la fecha de registro (2025-10-20) es del d√≠a de hoy. No se crear√° la Delivery Note.\n",
            "\n",
            "--- ETAPA 3: BUSCANDO SUGERENCIAS HEUR√çSTICAS (ERRORES DE TIPEO) ---\n",
            "18 series no tuvieron coincidencia exacta. Iniciando b√∫squeda heur√≠stica...\n",
            "\n",
            "ü§ñ Se generaron 0 sugerencias heur√≠sticas (distancia <= 2):\n",
            "\n",
            "\n",
            "üö® Se encontraron 18 series que pasaron el filtro de tiempo, pero no se encontraron en ninguna de las b√∫squedas:\n",
            " - ALCLB26BE330     - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - FZ324060210174   - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - 4857544358E4FDAC - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - 485754431123F1AB - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - 48575443B78A17B1 - 51ROD637 (Pat. AC774WF) - QPS\n",
            " - 48575443B78A40B1 - M563 - QPS\n",
            " - 48575443B786F5B1 - 51ROD465 (Pat. AE835PS) - QPS\n",
            " - FZ324060291719   - 51ROD466 (Pat. OCQ811) - QPS\n",
            " - 48575443D23A58B3 - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - 48575443D2381EB3 - Faltante - QPS\n",
            " - 48575443D230E9B3 - Faltante - QPS\n",
            " - 48575443D23BB0B3 - 51ROD636 (Pat. AD833WB) - QPS\n",
            " - FZ324060348087   - Faltante - QPS\n",
            " - FZ324060346807   - 51ROD466 (Pat. OCQ811) - QPS\n",
            " - FZ324060346767   - 51ROD637 (Pat. AC774WF) - QPS\n",
            " - FZ324060346900   - 51ROD637 (Pat. AC774WF) - QPS\n",
            " - FZ324060346703   - 51ROD637 (Pat. AC774WF) - QPS\n",
            " - FZ324060346746   - 51ROD637 (Pat. AC774WF) - QPS\n",
            "\n",
            "‚úÖ Proceso de trazabilidad completado.\n"
          ]
        }
      ],
      "source": [
        "# === üöÄ EJECUCI√ìN PRINCIPAL DEL PROCESO DE TRAZABILIDAD ===\n",
        "\n",
        "def main(filtro_tiempo=False, send=1):\n",
        "    max_distancia_heuristica = 2 if filtro_tiempo else 2\n",
        "    modo_desc = \"series sin movimiento en 2 semanas\" if filtro_tiempo else \"todas las series con stock\"\n",
        "    print(f\"--- INICIANDO AN√ÅLISIS DE TRAZABILIDAD ---\")\n",
        "    print(f\"Modo de operaci√≥n: Buscando {modo_desc}.\")\n",
        "    print(f\"Distancia heur√≠stica m√°xima establecida en: {max_distancia_heuristica}\")\n",
        "\n",
        "    #series=[\"ALCLB26BE330\", \"48575443885A3FB0\"]\n",
        "    series=[]\n",
        "    if not descargar_crudo_completo() or not obtener_series_frappe(series):\n",
        "        print(\"\\nüõë Proceso detenido por error en la descarga de datos iniciales.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- ETAPA 1: FILTRANDO SERIES A BUSCAR ---\")\n",
        "    series_para_buscar = obtener_series_filtradas(filtro_tiempo=filtro_tiempo)\n",
        "    if not series_para_buscar:\n",
        "        print(\"\\nNo se encontraron series que cumplan con los criterios de filtro (con almac√©n no vac√≠o). El proceso ha finalizado.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- ETAPA 2: BUSCANDO COINCIDENCIAS EXACTAS Y CREANDO DELIVERY NOTES ---\")\n",
        "\n",
        "    # Obtener la fecha de hoy para la comparaci√≥n\n",
        "    fecha_hoy = datetime.now().date()\n",
        "\n",
        "    coincidencias_exactas = buscar_series_en_crudo(series_para_buscar)\n",
        "\n",
        "    for serie, fila_crudo in coincidencias_exactas.items():\n",
        "        print(f\"\\nProcesando coincidencia exacta para: {serie}\")\n",
        "        # Access the date column by its name instead of index\n",
        "        # Based on the previous code, the date is likely in the second column.\n",
        "        # You might need to adjust the key name if the column name is different in your CSV.\n",
        "        fecha_registro_str = fila_crudo.get('FECHA DE CIERRE') # Replace 'FECHA DE REGISTRO' with the actual column name for the date\n",
        "\n",
        "        if fecha_registro_str:\n",
        "            try:\n",
        "                fecha_registro = datetime.strptime(fecha_registro_str, '%d/%m/%Y').date()\n",
        "\n",
        "                if fecha_registro == fecha_hoy:\n",
        "                    print(f\"‚úÖ ¬°Atenci√≥n! Se encontr√≥ una posible Delivery Note para la serie {serie}, pero la fecha de registro ({fecha_registro}) es del d√≠a de hoy. No se crear√° la Delivery Note.\")\n",
        "                else:\n",
        "                    crear_delivery_note_por_serie(serie, fila_crudo, send)\n",
        "            except ValueError:\n",
        "                print(f\"‚ö†Ô∏è Error: No se pudo parsear la fecha '{fecha_registro_str}' para la serie {serie}. Se intentar√° crear la Delivery Note de todos modos.\")\n",
        "                crear_delivery_note_por_serie(serie, fila_crudo, send)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Advertencia: No se encontr√≥ la columna de fecha para la serie {serie}. Se intentar√° crear la Delivery Note de todos modos.\")\n",
        "            crear_delivery_note_por_serie(serie, fila_crudo, send)\n",
        "\n",
        "\n",
        "    # --- FIN DE MODIFICACI√ìN ---\n",
        "\n",
        "    print(\"\\n--- ETAPA 3: BUSCANDO SUGERENCIAS HEUR√çSTICAS (ERRORES DE TIPEO) ---\")\n",
        "    series_encontradas_exactas = set(coincidencias_exactas.keys())\n",
        "    series_no_encontradas = [s for s in series_para_buscar if s not in series_encontradas_exactas]\n",
        "\n",
        "    if series_no_encontradas:\n",
        "        print(f\"{len(series_no_encontradas)} series no tuvieron coincidencia exacta. Iniciando b√∫squeda heur√≠stica...\")\n",
        "        sugerencias_heuristicas = buscar_series_en_crudo_heuristico(series_no_encontradas, max_distancia=max_distancia_heuristica)\n",
        "\n",
        "        series_encontradas_heuristica = set(s['serie_buscada'] for s in sugerencias_heuristicas)\n",
        "\n",
        "        # Find series that were not found in either search\n",
        "        if filtro_tiempo==True:\n",
        "          series_sin_match = [s for s in series_no_encontradas if s not in series_encontradas_heuristica]\n",
        "          # Print the result\n",
        "\n",
        "          if series_sin_match:\n",
        "              print(f\"\\nüö® Se encontraron {len(series_sin_match)} series que pasaron el filtro de tiempo, pero no se encontraron en ninguna de las b√∫squedas:\")\n",
        "\n",
        "              # Abrir el archivo filtrado para buscar almacenes\n",
        "              with open(archivo_seriesFiltrados, newline='', encoding=\"utf-8\") as f:\n",
        "                  reader = csv.DictReader(f)\n",
        "                  almacen_por_serie = {fila[\"serie\"]: fila[\"almacen\"] for fila in reader}\n",
        "\n",
        "              # Mostrar cada serie con su almac√©n\n",
        "              for s in series_sin_match:\n",
        "                  almacen = almacen_por_serie.get(s, \"N/A\")\n",
        "                  print(f\" - {s:<16} - {almacen}\")\n",
        "\n",
        "\n",
        "          else:\n",
        "              print(\"\\nüéâ Todas las series que pasaron el filtro de tiempo fueron encontradas (ya sea con coincidencia exacta o heur√≠stica).\")\n",
        "\n",
        "    else:\n",
        "        print(\"Todas las series filtradas fueron encontradas con coincidencia exacta. No es necesaria la b√∫squeda heur√≠stica.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Proceso de trazabilidad completado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*56)\n",
        "    print(\"|| EJECUTANDO AN√ÅLISIS DE SERIES CON ALMAC√âN NO VAC√çO ||\")\n",
        "    print(\"=\"*56 + \"\\n\")\n",
        "    main(filtro_tiempo=False, send=enviar)\n",
        "    main(filtro_tiempo=True, send=enviar)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRxEj6HjY2uebuccnK05qF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}